<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
>

    <channel>
        <title>ECNU ICA-LK</title>
        <atom:link href="%7balternate%20%7bRSS%20application/rss&#43;xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://localhost:1313/index.xml%7d" rel="self" type="application/rss+xml" />
        <link>http://localhost:1313/</link>
        <managingEditor>ecnu-cs-ica</managingEditor>
        <description>A simple, performance-first, SEO-friendly Hugo theme</description>
        <lastBuildDate>Thu, 13 Oct 2022 10:34:49 +0800</lastBuildDate>
        <language>zh-cn</language>
        <generator>Hugo -- gohugo.io</generator><item>
            <title>一种多模态融合方法PreFIL</title>
            <link>http://localhost:1313/2022/10/13/%E4%B8%80%E7%A7%8D%E5%A4%9A%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95prefil.html/</link>
            <pubDate>Thu, 13 Oct 2022 10:34:49 +0800</pubDate>
            <guid>http://localhost:1313/2022/10/13/%E4%B8%80%E7%A7%8D%E5%A4%9A%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95prefil.html/</guid><description>

&lt;h1 class=&#34;group &#34; id=&#34;answering-questions-about-data-visualizations-using-efficient-bimodal-fusion&#34;
    &gt;Answering Questions about Data Visualizations using Efficient Bimodal Fusion&lt;a href=&#34;#answering-questions-about-data-visualizations-using-efficient-bimodal-fusion&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h1&gt;



&lt;h3 class=&#34;group &#34; id=&#34;abstract&#34;
    &gt;Abstract：&lt;a href=&#34;#abstract&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;Chart question answering (CQA) is a newly proposed visual question answering (VQA) task where an algorithm must answer questions about data visualizations, e.g. bar charts, pie charts, and line graphs. CQA requires capabilities that natural-image VQA algorithms lack: fine-grained measurements, optical character recognition, and handling out-of-vocabulary words in both questions and answers. Without modifications, state-of-the-art VQA algorithms perform poorly on this task. Here, we propose a novel CQA algorithm called parallel recurrent fusion of image and language (PReFIL). PReFIL first learns bimodal embeddings by fusing question and image features and then intelligently aggregates these learned embeddings to answer the given question. Despite its simplicity, PReFIL greatly surpasses state-of-the art systems and human baselines on both the FigureQA and DVQA datasets. Additionally, we demonstrate that PReFIL can be used to reconstruct tables by asking a series of questions about a chart.&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;1introduction&#34;
    &gt;1.introduction&lt;a href=&#34;#1introduction&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;图表QA（CQA）是一项VQA任务，涉及回答有关数据可视化的问题。从形式上讲，给定数据可视化图像I和关于I的问题Q，CQA模型必须预测答案a。CQA需要理解图像中不同“符号”（图表中的元素）之间的关系。与自然图像相比，即使对图像进行微小的修改，也会导致正确答案的剧烈变化，这使得CQA成为研究推理机制的极好平台。CQA通常需要光学字符识别（OCR）和处理给定可视化所特有的单词。&lt;/p&gt;
&lt;p&gt;本文描述了一种新的算法，称为图像与语言并行递归融合（PReFIL）。PReFIL通过使用低级和高级图像特征共同学习双模嵌入，这使得它能够回答需要多步骤推理和比较的复杂问题，而无需使用专门的关系或注意模块。大量实验表明，在两个具有挑战性的CQA数据集中，我们的算法性能优于当前最先进的方法。&lt;/p&gt;
&lt;p&gt;我们的主要贡献是：&lt;/p&gt;
&lt;p&gt;• 严格审查了现有的CQA数据集，概述了它们的优缺点（第2.1节）。&lt;br/&gt; 
• 使用众包方式收集DVQA数据集的人的表现（第4节）&lt;br/&gt; 
• 提出了一种新的算法，称为并行递归早期图像与语言融合（PReFIL）（第3节）。PReFIL在CQA数据集上大大超过了现有方法，在DVQA和FigureQA上也优于人类（第4节）。PReFIL的代码和预先培训的模型将公开发布。&lt;br/&gt; 
• 率先使用迭代式问答从图表中重建表格（第4.4节）&lt;br/&gt; 
• 根据结果，概述了创建更具挑战性的数据集和算法以理解数据可视化的路线图（第5节）。&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;2related-work&#34;
    &gt;2.Related work&lt;a href=&#34;#2related-work&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;



&lt;h4 class=&#34;group &#34; id=&#34;21两个数据集dvqa和figureqa&#34;
    &gt;2.1两个数据集DVQA和FigureQA。&lt;a href=&#34;#21%e4%b8%a4%e4%b8%aa%e6%95%b0%e6%8d%ae%e9%9b%86dvqa%e5%92%8cfigureqa&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;如图所示。&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;211dvqa与figureqa&#34;
    &gt;2.1.1DVQA与FigureQA&lt;a href=&#34;#211dvqa%e4%b8%8efigureqa&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;DVQA和FigureQA各有优缺点。我们在下面对它们进行比较和对比。共同优势：两个数据集都很大，并提供足够的训练样本来训练大型模型。这两个数据集都为问答对之外的所有图形元素提供了详细的注释，这两个数据集的创建者都试图消除一些偏见的来源。最后，这两个数据集都提供了简单和困难测试拆分，其中困难测试拆分衡量的泛化程度超出了培训期间看到的范围。但DVQA的优势大于FigureQA。共同限制：作为合成生成的数据集，DVQA和FigureQA都忽略了现实世界数据可视化中的许多可变性。所有DVQA的图表都是用Matplotlib制作的，所有FigureQA的都是用Bokeh制作的。FigureQA仅使用通用标题和其他图表元素。DVQA有一些种类，但最终仅限于几个模板。虽然问题可能很复杂，但它们缺乏人工生成的查询的多样性。&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;22现有cqa算法&#34;
    &gt;2.2现有CQA算法&lt;a href=&#34;#22%e7%8e%b0%e6%9c%89cqa%e7%ae%97%e6%b3%95&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;SANDY使用了叠加注意力网络（SAN）的改进版本，该版本已广泛用于VQA。SAN使用这个问题来关注卷积特征映射。它无法处理DVQA测试集中的OOV单词或其问题和答案中的图表特定单词。为了解决这个问题，SANDY使用现成的OCR方法来识别这些单词，并引入动态编码来表示OOV和图表特定的单词。SANDY的OCR动态编码方案可以合并到任何基于分类的VQA算法中。&lt;br/&gt;
FigureQA的创建者在他们的数据集上使用了关系网络（RN）。RN编码图像中每对“对象”之间的成对交互，使其能够回答涉及关系的问题。每个“对象”都是卷积特征映射的一个单元。在CLEVR中，RN被证明在组合推理方面特别有效，它超过了图QA中的基线。&lt;br/&gt;
FigureNet是由不同模块组成的FigureQA的多步骤算法。第一个模块称为光谱分离器，用于识别图表的元素和颜色。然后是提取模块，该模块量化每个元素表示的值。然后将其与前馈网络一起用于预测答案。FigureNet使用FigureQA图表元素的详细注释对每个模块进行预培训。因为FigureNet依赖于访问每个图表元素的度量值，所以他们只能将其应用于FigureNet的条形图和饼图。&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;3prefil模型&#34;
    &gt;3.PReFIL模型&lt;a href=&#34;#3prefil%e6%a8%a1%e5%9e%8b&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;我们提出了用于CQA的PReFIL算法。如图3所示，PReFIL有两个平行的Q+I融合分支。每个分支都从40层DenseNet的两个位置获取问题特征（来自LSTM）和图像特征，即低级特征（来自第14层）和高级特征（来自于第40层）。每个Q+I融合块将问题特征连接到卷积特征图的每个元素，然后有一系列1×1卷积来创建问题特定的双模嵌入。这些嵌入被反复聚合，然后被送入预测答案的分类器。尽管PReFIL由相对简单的元素组成，但其性能优于使用RN和注意机制的更复杂方法。&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;31多阶段图像编码器&#34;
    &gt;3.1多阶段图像编码器&lt;a href=&#34;#31%e5%a4%9a%e9%98%b6%e6%ae%b5%e5%9b%be%e5%83%8f%e7%bc%96%e7%a0%81%e5%99%a8&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;对于所有模型，图像编码器都是经过从头开始培训的DenseNet。DenseNet是训练深度卷积神经网络（CNN）的有效架构。它由几个“dense blocks”和dense blocks之间的“transition blocks”组成。每个dense block有几个卷积层，其中每个层使用前面所有层的输出作为其输入。transition blocks位于两个密集块之间，通过卷积和池来改变特征图大小。这种体系结构鼓励功能重用，改进训练，并减轻逐渐消失的梯度，使培训非常深入的网络变得容易。功能重用允许DenseNet学习复杂的视觉功能，与其他架构相比，参数更少。&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;在深层CNN中，复杂特征作为视觉特征的层次结构来学习，早期层学习简单特征，后期层学习更高级的特征，这些特征是简单特征的组合。在数据可视化中，诸如色块、线条、纹理等简单特征传达的重要信息通常被CNN的深层抽象掉。因此，我们在模型中使用了低卷积和高卷积特征，这两种特征都与使用LSTM学习的问题嵌入一起被送入并行融合模块。&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;32图像与语言的并行融合&#34;
    &gt;3.2图像与语言的并行融合&lt;a href=&#34;#32%e5%9b%be%e5%83%8f%e4%b8%8e%e8%af%ad%e8%a8%80%e7%9a%84%e5%b9%b6%e8%a1%8c%e8%9e%8d%e5%90%88&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;使用视觉和语言特征联合调整视觉特征可以让模型为下游任务学习更丰富的特征。我们的Q+I融合块首先将所有输入卷积特征图的空间位置与问题特征连接起来，然后使用一系列使用1×1卷积的层进行双模融合。这允许问题调整视觉特征处理，并产生从图像和问题中捕获信息的双模嵌入。这种方法类似于早期的VQA模型，它将CNN嵌入连接到问题嵌入，关键的区别在于这发生在整个场景的空间池之前。我们并行地对低级和高级卷积特征执行此操作。&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;33双模特征的循环聚合&#34;
    &gt;3.3双模特征的循环聚合&lt;a href=&#34;#33%e5%8f%8c%e6%a8%a1%e7%89%b9%e5%be%81%e7%9a%84%e5%be%aa%e7%8e%af%e8%81%9a%e5%90%88&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;在CNN中，从特征图F中聚合信息的最常用方法∈ RM×N×D是通过平均池或最大池来跨空间维度折叠以生成D维向量。另一种方法是“展平”F，将其转换为DM N维向量。最近关注的方法使用加权和进行了探索，其中每个区域的相对重要性都基于这个问题。这些方法可能无法捕获功能之间的交互，特别是对于诸如问答之类的高级任务。为了解决这个问题，我们使用双向选通递归单元（biGRU）来聚合信息，该单元依次从F中的每个MN位置获取D维特征。聚合的特征被发送到分类器以预测答案。&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;34dvqa数据集的ocr集成&#34;
    &gt;3.4DVQA数据集的OCR集成&lt;a href=&#34;#34dvqa%e6%95%b0%e6%8d%ae%e9%9b%86%e7%9a%84ocr%e9%9b%86%e6%88%90&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;与FigureQA和大多数VQA任务不同，DVQA需要OCR来回答其推理和数据问题。一个由训练中看到的所有单词组成的固定词汇表是不够的，因为模型在测试过程中会遇到OOV单词。为了将OCR集成到PReFIL中，我们使用与SANDY模型相同的动态编码方案。动态编码创建一个特定于图像的字典，该字典将场景元素的空间位置与字典中的条目相关联。在运行网络之前，使用OCR检测所有单词，然后根据每个单词的空间位置将其与动态编码字典中的相应元素相关联。随后，如果遇到动态词典中的疑问词，则相应的元素设置为1。对于答案，将为动态编码输出保留一部分分类层。&lt;br/&gt;
为了评估OCR的影响，我们测试了三个OCR版本以及一个未经动态编码训练的算法版本，即，仅使用从列车分割构造的固定词汇表。前两个OCR系统：一个是oracle（完美）OCR模型，另一个是使用Tesseract的真实OCR系统。&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;4实验和结果&#34;
    &gt;4.实验和结果&lt;a href=&#34;#4%e5%ae%9e%e9%aa%8c%e5%92%8c%e7%bb%93%e6%9e%9c&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;



&lt;h4 class=&#34;group &#34; id=&#34;41figureqa&#34;
    &gt;4.1FigureQA&lt;a href=&#34;#41figureqa&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;FigureQA有两个验证集和两个非公开可用的测试集。验证1和测试1的颜色与训练集相同，验证2和测试2的颜色方案与训练不同。&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;42dvqa&#34;
    &gt;4.2DVQA&lt;a href=&#34;#42dvqa&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;DVQA分为Test Familifier（测试熟悉）和Test Novel（测试新颖），前者包含带单词的条形图，后者也包含在Train set中遇到的单词。表4给出了两个DVQA分割的结果。&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;43消融实验&#34;
    &gt;4.3消融实验&lt;a href=&#34;#43%e6%b6%88%e8%9e%8d%e5%ae%9e%e9%aa%8c&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;我们通过分析一系列消融实验来研究PReFIL组分的贡献。我们在只有500000个随机选择的训练样本的DVQA子集上对每个模型变体和原始PReFIL（Oracle OCR）进行了25轮的训练。消融模型为：&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;


&lt;h4 class=&#34;group &#34; id=&#34;44通过提问重建表格&#34;
    &gt;4.4.通过提问重建表格&lt;a href=&#34;#44%e9%80%9a%e8%bf%87%e6%8f%90%e9%97%ae%e9%87%8d%e5%bb%ba%e8%a1%a8%e6%a0%bc&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h4&gt;

&lt;p&gt;作为PReFIL的应用，我们介绍了DVQA的表重建。DVQA的问题模板通过反复询问每个图表的问题，提供了完全重构条形图所需的问题。我们的方法在算法1中给出。重建示例如图4所示，使用PReFIL（Oracle OCR）的结果如表6所示。形状预测可以以近乎完美的精度完成，但标签和值预测的性能都会下降。为了研究图表重建中不同组件的准确性，我们还报告了迭代问答的三个主要组件的准确性：1）形状预测：关于图片中条形和图例数量的问题；2） 标签预测：预测给定条或图例的标签；和3）V值预测：预测给定条的值。&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;5讨论&#34;
    &gt;5.讨论&lt;a href=&#34;#5%e8%ae%a8%e8%ae%ba&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;所有OCR版本在结构问题上都超过了人类基线，但只有使用oracle OCR的PReFIL在所有问题类型上超过了人类。我们发现更好的OCR方法可以为DVQA带来更好的结果。OCR技术的未来发展可能会进一步改善PReFIL。&lt;/p&gt;


&lt;h3 class=&#34;group &#34; id=&#34;6结论&#34;
    &gt;6.结论&lt;a href=&#34;#6%e7%bb%93%e8%ae%ba&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h3&gt;

&lt;p&gt;提出了PReFIL这一新的CQA系统，它改进了最先进的技术，并在两个数据集上超越了人类的准确性。与其他VQA任务一样，我们的结果表明需要更难的数据集。对于CQA来说，更好的OCR对于推进该领域也很重要。我们的工作有潜力改进从图表中检索信息，图表有许多应用，包括自动信息检索、表格重建，以及帮助有视觉障碍的人更好地理解图表。&lt;/p&gt;
</description></item><item>
            <title>SSL VQA</title>
            <link>http://localhost:1313/2022/10/06/ssl-vqa.html/</link>
            <pubDate>Thu, 06 Oct 2022 17:27:49 +0800</pubDate>
            <guid>http://localhost:1313/2022/10/06/ssl-vqa.html/</guid><description>

&lt;h1 class=&#34;group &#34; id=&#34;ssl-vqa&#34;
    &gt;SSL-VQA&lt;a href=&#34;#ssl-vqa&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;QICE&lt;/strong&gt;：可以迫使模型参考图像内容，而不是盲目回答。为此，我们提出了一个称为问题图像相关性估计（QICE）的辅助任务，即二元分类任务，用于在回答问题之前预测问题图像对是否相关。在本文中，我们定义了相关问题图像对，因为图像可以用来回答具有特定答案的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;平衡问题-图像对&lt;/strong&gt;：通过替换（Q，I）问答对中的I，（Q，I）为相关问答对c=1，（Q，I’）为不相关问答对c=0。最终得到的数据集有一半是相关，一般是无关。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;相关性估计&lt;/strong&gt;：利用生成的平衡数据，我们可以训练QICE模型，通过优化交叉熵损失来预测每个问题图像对的相关标签。
&lt;img src=&#34;1.png&#34; width=&#34;1600px&#34; height=&#34;200px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lself可以被解释为自监督训练损失，因为它仅利用我们生成的数据中的标签监督c。目标函数确保QICE模型能够理解问题和图像内容，因为每个Q对应于平衡的相关和无关实例，并且不依赖语言先验。在下一小节中，我们将讨论如何利用我们的辅助任务QICE和平衡数据，以帮助VQA模型在统一框架中消除语言偏见。&lt;/p&gt;
&lt;p&gt;在本节中，我们提出了一个统一的VQA框架，可以在训练期间同时回答问题和估计问题图像相关性。显然，上面定义的QICE任务可以与VQA共享相同的网络结构，因为它们具有完全相同的输入和类似的输出：它们都将问题图像对（I，Q）作为输入，VQA预测答案空间a上的分布，而QICE生成特定答案a上的二进制标签。这种特性促使我们在统一的VQA框架中同时解决这两个任务，如图2所示。&lt;/p&gt;
&lt;img src=&#34;图片2.png&#34; width=&#34;1600px&#34; height=&#34;200px&#34; /&gt;
&lt;p&gt;对于图2（a）所示的VQA模型，它将相关问题图像对（Q，I）作为输入，并预测答案空间a上的分布F（a|Q，I），可通过最小化VQA损失Lvqa ce或Lvqa ml进行优化。该目标函数教导模型学习回答问题的能力。对于图2（c）中显示的QICE，给定对应于特定答案a的问题图像对（I，Q），VQA模型的预测概率P（a|Q，I）可被视为（I，Q）是相关对的置信度。概率越大，匹配度越高。因此，Lself可以重写为：&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;该模型需要对问题图像相关性估计任务进行正确的二值预测，这可以加强模型以更好地理解图像，因为每个问题都与等量的相关和不相关图像配对。更具体地说，Lself的第一项旨在最大化问题图像对相关的置信度，这与VQA任务的目标一致，该任务以高置信度预测地面真值A。&lt;/p&gt;
&lt;p&gt;最重要的是，Lself的第二项被设计为最小化一对相关的置信度，这完全符合语言先验约简。直观地说，VQA模型的问题依赖性可以通过给定不相关图像时正确回答问题的置信度来衡量。信心越大，依赖性越强。最小化无关对的置信度可以显式地防止VQA模型被语言先验过度驱动，这里我们将其命名为问题依赖性损失Lqd：&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;最小化P（A | Q，I’）和最小化−log(1 − P(A|Q, I’))等价。但在训练时最小化前者比最小化后者稳定。所以将公式更新为:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;因此，QICE任务自然可以被视为基础多任务学习，包含两个任务：原始VQA任务和语言先验约简任务。我们可以将Lself重新表述如下：&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;其中，Lvqa可以是任何VQA损失（Lvqa ce或Lvqa ml），α是超参数。显然，Lself可以被视为广义的VQA损失，因为当α=0时，它退化为Lvqa。这意味着问题依赖性损失Lqd实际上充当正则化子，防止VQA模型记忆语言先验并迫使其更好地理解图像。因此，Lself在控制回答问题和减少语言先验之间的平衡方面提供了灵活性。此外，我们不需要显式优化模型，使其成为估计问题图像对相关性的专家，我们只需要使用其平衡监督来补偿VQA中的偏见和自监督损失。在此之后，我们的方法可以在不使用外部监督的情况下以自我监督的方式减轻语言先验。&lt;/p&gt;
</description></item><item>
            <title>画廊（春）</title>
            <link>http://localhost:1313/2022/10/01/flower-gallery.html/</link>
            <pubDate>Sat, 01 Oct 2022 02:49:13 +0200</pubDate>
            <guid>http://localhost:1313/2022/10/01/flower-gallery.html/</guid><description>&lt;div class=&#34;gallery-box&#34;&gt;
    &lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;

&lt;/div&gt;



&lt;h2 class=&#34;group &#34; id=&#34;grid-布局&#34;
    &gt;Grid 布局&lt;a href=&#34;#grid-%e5%b8%83%e5%b1%80&#34;
        &gt;&lt;i class=&#34;eva eva-link ml-3 align-middle text-theme opacity-0 transition ease-in-out group-hover:opacity-100&#34;&gt;&lt;/i&gt;&lt;/a
&gt;&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;春，甘美之春，一年之中的尧舜，&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;gallery-grid m-3 columns-2 gap-3 sm:columns-3 md:m-4 md:columns-4 md:gap-4&#34;&gt;
    &lt;p&gt;




&lt;/p&gt;

&lt;/div&gt;

</description></item></channel>
</rss>